{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo-oml.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkh-DMFHRIuF"
      },
      "source": [
        "# Demo of pre-trained anime character identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj4VvGUD0v9j"
      },
      "source": [
        "! pip install git+https://github.com/kosuke1701/AnimeCV.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW9XwO591FKf"
      },
      "source": [
        "!wget https://github.com/kosuke1701/AnimeCV/releases/download/0111_best_randaug/0111_best_randaug.zip\n",
        "!unzip 0111_best_randaug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCaMRmrW3fn-"
      },
      "source": [
        "# Face detection module\n",
        "from animecv.object_detection import FaceDetector_EfficientDet\n",
        "from animecv.util import load_image\n",
        "\n",
        "detector = FaceDetector_EfficientDet(coef=2, use_cuda=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vBSVgTz1O-5"
      },
      "source": [
        "# Character face encoder\n",
        "import animecv\n",
        "from animecv.module import ImageBBEncoder, Similarity\n",
        "from torchvision import transforms\n",
        "\n",
        "torch_model = animecv.general.OML_ImageFolder_Pretrained(\"0111_best_randaug\")\n",
        "transform = [\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "]\n",
        "transform = transforms.Compose(transform)\n",
        "\n",
        "encoder = ImageBBEncoder(torch_model, post_trans=transform, scale=1.0)\n",
        "encoder.to(\"cuda\")\n",
        "\n",
        "threshold = 0.65 # Threshold of dot-product of embeddings which is determined so that the model's FPR becomes 0.22."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLLV-6C61mKu"
      },
      "source": [
        "from google.colab import files\n",
        "import IPython\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Upload your image here. Two images are required.\n",
        "\n",
        "ここで画像をアップロードしてください。合計２枚の画像をアップロードします。"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JGIv8Jq1quV"
      },
      "source": [
        "uploaded = list(files.upload())\n",
        "image1 = uploaded[0]\n",
        "IPython.display.Image(image1, width=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L8n59rK5my8"
      },
      "source": [
        "uploaded = list(files.upload())\n",
        "image2 = uploaded[0]\n",
        "IPython.display.Image(image2, width=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSFAdQt99AMY"
      },
      "source": [
        "images = [load_image(image1), load_image(image2)]\n",
        "face_bbox = detector.detect(images)\n",
        "face_embs, lst_i_img, lst_i_bbox = encoder.encode(images, face_bbox)\n",
        "face_embs = face_embs.detach().cpu().numpy()\n",
        "\n",
        "cropped_images = []\n",
        "for i_img, i_bbox in zip(lst_i_img, lst_i_bbox):\n",
        "    xmin, ymin, xmax, ymax = face_bbox[i_img][i_bbox][\"coordinates\"]\n",
        "    crop_img = images[i_img].crop((xmin, ymin, xmax, ymax))\n",
        "\n",
        "    if min(crop_img.size) == 0:\n",
        "        continue\n",
        "\n",
        "    cropped_images.append(crop_img)\n",
        "\n",
        "n_img = len(cropped_images)\n",
        "print(f\"Detected {n_img} faces.\")\n",
        "\n",
        "for i_img, img in enumerate(cropped_images):\n",
        "    ax = plt.subplot(1, n_img, i_img+1)\n",
        "    ax.imshow(np.array(img))\n",
        "plt.show()\n",
        "\n",
        "print(\"Similarity of each face pair. Rows and columns correspond to each image.\")\n",
        "for i_img in range(n_img):\n",
        "    line = []\n",
        "    for j_img in range(n_img):\n",
        "        sim = np.dot(face_embs[i_img], face_embs[j_img])\n",
        "        label = \"SAME\" if sim > threshold else \"DIFF\"\n",
        "        line.append(f\"{sim:.3f}/{label}\")\n",
        "    print(\"\\t\".join(line))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}